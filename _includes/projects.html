<section class="section projects-section">
                <h2 class="section-title"><i class="fa fa-archive"></i>Projects</h2>
                <div class="intro">
                    <p>Over the past few years I have worked on the following academic and personal projects. They are predominently in the fields of data science, machine learning and big data analytics. For work samples, click the respective titles.</p>
                </div><!--//intro-->
                <div class="item">
                    <span class="project-title"><a href="#hook">HR Predictive Modeling Data Analytics| Tools Used: Python, R, plotly, Excel and Tableau</a></span> <br>
					<span class="project-tagline">
					•	Applied Predictive Data Analytics on Attrition, Abseenteeism and Time to hire data from Continental AG. Used R and Excel for Data cleaning.<br>
					•	Decision Trees, Logistic Regression and Random Forest were implemented on the data to come up with models and predictions.<br>
					•	An Interactive dashboard with python framework dash by plotly was coded and deployed and ran on Heroku for external access. <a href = "https://my-dash-hrpred2.herokuapp.com/"> Click here to view the dashboard<br></a>
					<b><i>Techniques Used: Exploratory Data Analysis, Predictive Analysis, Regression Analysis, Decision Trees, Plotly & Tableau Visualization.</b></i>
					</span>
                </div><!--//item-->
				
				
                <div class="item">
                    <span class="project-title"><a href="#hook">Surprising Discoveries for Online Health Information| Tools Used: Python, Jupyter Notebook, genism, tensorflow</a></span> <br>
					<span class="project-tagline">
					•	A pre-defined word vector trained on medical documents was obtained and cosine similarity between the sentence vectors were calculated.<br>
					•	An API based on distributional similarity, Latent Semantic Analysis and semantic relations extracted from wordnet was used to find the similarity between two sentences, 25% of maximum cosine similarity in a document was set as threshold and surprising scores were calculated.<br>
					•	An LDA model was constructed  top 20 topics were extracted; also Word2Vec training was done to come up with word embeddings.<br>
					<b><i>Techniques Used: Exploratory Data Analysis, Cosine Similarity, Latent Dirichlet Allocation, Latent Semantic Analysis & Word Embedding.</b></i>
					</span>
                </div><!--//item-->
				
				
                <div class="item">
                    <span class="project-title"><a href="#hook">Hire Heroes USA Client Management| Tools Used: SAS, R, Excel and Tableau</a></span> <br>
					<span class="project-tagline">
					•	Applied Big Data and Analytics techniques to help a non-profit organization HHUSA, better understand and optimize factors that affect their client management process, staff activities and the employment opportunities offered to veterans.<br>
					•	Text mining was used to generate features and predictive modelling techniques were used to model the quantities of interest.<br>
					<b><i>Techniques Used: Exploratory Data Analysis, Predictive Analysis, Regression Analysis, Text Mining, Decision Trees, Tableau Visualization.</b></i>
					</span>
                </div><!--//item-->
				
				
                <div class="item">
                    <span class="project-title"><a href="#hook">Indexing Wikidumps on Cloud Platforms| Technologies Used: Elasticsearch, Logstash, Kibana, SOLR, Deep Neural Network</a></span> <br>
					<span class="project-tagline">
					•	Implemented ELK Stack on Google Cloud Platform and Amazon Web Service and SOLR on Microsoft Azure.<br>
					•	Designed a Chatbot that allows users to have easy interactions and for fast retrieval of Wikipedia documents using ELK Stack and SOLR.
					</span>
                </div><!--//item-->
				
				
                <div class="item">
                    <span class="project-title"><a href="#hook">Twitter Text Analysis - Movies Success| Techniques & Tools Used: Python, TweepyAPI, NLTK, matplotlib                   </a></span> <br>
					<span class="project-tagline">
					•	Tweets crawled using the Tweepy API in Python were pre-processed to create a corpus for analysis using NLTK module.<br>
					•	Sentiment Analysis was then performed on the corpus to understand, the sentimental influence of that movie and a tag cloud of top 50 words in the tweets was created to understand the audience sentiments about the movie.
					</span>
                </div><!--//item-->
				
				
                <div class="item">
                    <span class="project-title"><a href="#hook">An Electronic Medical Record for an Outpatient Clinic| Tools Used: MySQL</a></span> <br>
					<span class="project-tagline">
					•	Designed and developed a complete OLTP database for an Outpatient Clinic that can efficiently store, retrieve, manipulate, and query records. <br>
					•	Implemented Authentication and Role based access control to all the data tables & used views and indexes for easy data access.
					</span>
                </div><!--//item-->
				

            </section><!--//section-->
